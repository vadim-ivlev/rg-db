{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export CSV files to Elasticsearch\n",
    "\n",
    "Change input_file_name, elastic_endpoint, index_name, max_number, batch_size variables here:\n",
    "\n",
    "To access notebook via ssh: `$ ssh -N -L 8888:localhost:8888 {user}@{server_ip}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting psycopg2-binary\n",
      "  Downloading psycopg2_binary-2.8.6-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 730 kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
      "Successfully installed psycopg2-binary-2.8.6\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import psycopg2\n",
    "import psycopg2.extras  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "username = 'admin'\n",
    "password = os.getenv('RGPASS')\n",
    "DSN = os.getenv('RGDSN')\n",
    "print(DSN)\n",
    "# input directory\n",
    "source_dir = \"/home/jovyan/work/csv/\"\n",
    "\n",
    "# elastic endpoint\n",
    "elastic_endpoint = \"http://rg-corpus-caddy:8080/elasticsearch/\"\n",
    "\n",
    "\n",
    "\n",
    "def save_batch(lines: list, elastic_endpoint:str, index_name:str):\n",
    "    \"\"\"saves batch of lines to database\"\"\"\n",
    "    data = ''.join(lines)\n",
    "#     print(data)\n",
    "#     print('----------------------------------')\n",
    "    r = requests.post(f'{elastic_endpoint}{index_name}/_bulk', \n",
    "                      headers = {'Content-Type': 'application/x-ndjson; charset=UTF-8'}, \n",
    "                      auth=(username,password),\n",
    "                      data=data.encode('utf-8'))\n",
    "    try:\n",
    "        rjson=r.json()\n",
    "        if rjson.get('errors') is not False:\n",
    "            pprint(rjson)\n",
    "    except:\n",
    "        pprint(r)\n",
    "        \n",
    "    lines.clear()\n",
    "    \n",
    "    \n",
    "def save_csv_file_to_elastic(input_file_name: str, elastic_endpoint:str, index_name:str,  max_number=0, batch_size=1000):\n",
    "    \"\"\"Saves CSV file to elasticsearch\n",
    "    - input_file_name - name of CSV file\n",
    "    - max_number - max number of records to save\n",
    "    - batch_size  - number of records in a batch \n",
    "    \"\"\"\n",
    "    # to process long fields in CSS file\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    \n",
    "    counter =0    # aka record id \n",
    "    lines =[]     # list of text lines to save\n",
    "    \n",
    "    with open(input_file_name) as input_file:\n",
    "        reader = csv.DictReader(input_file)\n",
    "\n",
    "        for row in reader:\n",
    "            if counter >= max_number: break\n",
    "            lines.append('{ \"index\" : {\"_id\" : \"'+str(counter)+'\" } }\\n')\n",
    "            lines.append(json.dumps(row, ensure_ascii=False)+'\\n')\n",
    "            counter += 1\n",
    "            if counter % batch_size ==0:\n",
    "                print(f'counter = {counter}----------------')\n",
    "                save_batch(lines, elastic_endpoint, index_name)\n",
    "                \n",
    "        print(f'counter = {counter}----------------')\n",
    "        save_batch(lines, elastic_endpoint, index_name)\n",
    "\n",
    "def save_table_to_elastic(table_name: str, elastic_endpoint:str, index_name:str,  max_number=0, batch_size=1000):\n",
    "    \"\"\"Saves table to elasticsearch\n",
    "    - table_name - name of postgres table\n",
    "    - max_number - max number of records to save\n",
    "    - batch_size  - number of records in a batch \n",
    "    \"\"\"\n",
    "    # to process long fields in CSS file\n",
    "    csv.field_size_limit(sys.maxsize)\n",
    "    \n",
    "    \n",
    "    counter =0    # aka record id \n",
    "    lines =[]     # list of text lines to save\n",
    "    \n",
    "    \n",
    "    conn = psycopg2.connect(DSN)\n",
    "    try:\n",
    "        # connection usage\n",
    "        pass\n",
    "    finally:\n",
    "        conn.close()    \n",
    "    \n",
    "#     with open(input_file_name) as input_file:\n",
    "#         reader = csv.DictReader(input_file)\n",
    "\n",
    "#         for row in reader:\n",
    "#             if counter >= max_number: break\n",
    "#             lines.append('{ \"index\" : {\"_id\" : \"'+str(counter)+'\" } }\\n')\n",
    "#             lines.append(json.dumps(row, ensure_ascii=False)+'\\n')\n",
    "#             counter += 1\n",
    "#             if counter % batch_size ==0:\n",
    "#                 print(f'counter = {counter}----------------')\n",
    "#                 save_batch(lines, elastic_endpoint, index_name)\n",
    "                \n",
    "#         print(f'counter = {counter}----------------')\n",
    "#         save_batch(lines, elastic_endpoint, index_name)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверки соединения и наличия csv файлов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'name': '61eeb9f222df',\n",
       " 'cluster_name': 'docker-cluster',\n",
       " 'cluster_uuid': 'MfRelN7QTXedUsrgMLejWA',\n",
       " 'version': {'number': '7.7.0',\n",
       "  'build_flavor': 'default',\n",
       "  'build_type': 'docker',\n",
       "  'build_hash': '81a1e9eda8e6183f5237786246f6dced26a10eaf',\n",
       "  'build_date': '2020-05-12T02:01:37.602180Z',\n",
       "  'build_snapshot': False,\n",
       "  'lucene_version': '8.5.1',\n",
       "  'minimum_wire_compatibility_version': '6.8.0',\n",
       "  'minimum_index_compatibility_version': '6.0.0-beta1'},\n",
       " 'tagline': 'You Know, for Search'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      " obj_id,announce,authors,date_modified,full_text,images,index_priority,is_active,is_announce,is_paid,link_title,links,obj_kind,projects,release_date,spiegel,title,uannounce,url,migration_status,process_status,lemmatized_text,entities_text,entities_grouped\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# проверки\n",
    "r = requests.get(elastic_endpoint, auth=(username, password))\n",
    "display(r.status_code)\n",
    "try: display(r.json())\n",
    "except:pass\n",
    "with open(source_dir+'articles.csv') as f:\n",
    "    print('----------\\n',f.readline())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт csv в Эластик "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 7.39 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# save_csv_file_to_elastic(source_dir+'rubrics.csv', elastic_endpoint, 'rubrics', 2000 , 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_csv_file_to_elastic(source_dir+'rubrics_objects.csv', elastic_endpoint, 'rubrics_objects', 5000000 , 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dockertest Wall time: 21min 43s \n",
    "save_csv_file_to_elastic(source_dir+'articles.csv', elastic_endpoint, 'articles', 1250000 , 5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
