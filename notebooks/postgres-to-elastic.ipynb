{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Экспорт таблиц из Postgres в Elasticsearch\n",
    "========================\n",
    "\n",
    "Таблицы копируются как есть, без изменений в базе данных Postgres.\n",
    "\n",
    "\n",
    "To access notebook via ssh: `$ ssh -N -L 8888:localhost:8888 {user}@{server_ip}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import sys\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import psycopg2\n",
    "import psycopg2.extras  \n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "import datetime\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "host=134.0.107.93 port=5432 dbname=rgdb user=root password=rosgas2011 sslmode=disable\nrosgas2011\n"
     ]
    }
   ],
   "source": [
    "print(os.getenv('RGDSN'))\n",
    "print(os.getenv('RGPASS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RGUSER = 'admin'\n",
    "RGPASS = os.getenv('RGPASS')\n",
    "RGDSN = os.getenv('RGDSN')\n",
    "# ELASTIC_ENDPOINT = \"http://rg-corpus-caddy:8080/elasticsearch/\"\n",
    "ELASTIC_ENDPOINT = 'http://dockertest.rgwork.ru:9094/elasticsearch/' if os.path.exists('local-file.txt') else 'http://es01:9200/'\n",
    "es = Elasticsearch(ELASTIC_ENDPOINT, http_auth=(RGUSER, RGPASS))\n",
    "\n",
    "\n",
    "\n",
    "def save_batch(lines: list, elastic_endpoint:str, index_name:str):\n",
    "    \"\"\"saves batch of lines to database\"\"\"\n",
    "    data = '\\n'.join(lines)+'\\n'\n",
    "    r = requests.post(f'{elastic_endpoint}{index_name}/_bulk', \n",
    "                      headers = {'Content-Type': 'application/x-ndjson; charset=UTF-8'}, \n",
    "                      auth=(RGUSER,RGPASS),\n",
    "                      data=data.encode('utf-8'))\n",
    "    try:\n",
    "        rjson=r.json()\n",
    "        if rjson.get('errors') is not False:\n",
    "            pprint(rjson)\n",
    "    except:\n",
    "        pprint(r)\n",
    "        \n",
    "    \n",
    "def set_progress(p1,p2, val):\n",
    "    if p1 is not None:\n",
    "        p1.value = val\n",
    "    if p2 is not None:\n",
    "        p2.value = str(val)\n",
    "\n",
    "def save_table_to_elastic(table_name: str, idname: str, elastic_endpoint:str, index_name:str,  max_number=0, batch_size=1000):\n",
    "    \"\"\"Копировать таблицу в elasticsearch, как есть, без изменений в базе данных Postgres.\n",
    "    \n",
    "    - table_name - name of postgres table\n",
    "    - idname - выражение для поля postgres значение которого нужно сделать уникальным идентификатором записи Эластик\n",
    "    - elastic_endpoint - конечная точка эластик\n",
    "    - index_name - имя инедекса эластик\n",
    "    - max_number - max number of records to save\n",
    "    - batch_size  - number of records in a batch \n",
    "    \"\"\"\n",
    "    p1 = widgets.IntProgress(min=0, max=max_number) \n",
    "    p2 = widgets.Label()\n",
    "    box = widgets.HBox([p1,p2])\n",
    "    display(box)\n",
    "    \n",
    "    start = time.time()\n",
    "    counter =0    # aka record id \n",
    "    lines =[]     # list of text lines to save\n",
    "    conn = psycopg2.connect(RGDSN)\n",
    "    try:\n",
    "        with conn:\n",
    "            with conn.cursor('servercursor') as curs:\n",
    "                curs.execute(f\"SELECT {idname}, row_to_json(r,FALSE)::text FROM {table_name} r LIMIT {max_number}\")\n",
    "                for record in curs:\n",
    "                    if counter >= max_number: break\n",
    "                    elastic_id = counter if record[0] is None else record[0]\n",
    "                    lines.append('{\"index\" : {\"_id\" : \"'+str(elastic_id)+'\"}}')\n",
    "                    lines.append(record[1])\n",
    "                    counter += 1\n",
    "                    if counter % batch_size ==0:\n",
    "                        duration = (time.time()-start)/60\n",
    "                        p1.value=counter; p2.value = f'{counter}/{max_number}. Время {duration:.2f} мин'\n",
    "                        save_batch(lines, elastic_endpoint, index_name)\n",
    "                        lines.clear()\n",
    "                p1.value=counter; p2.value = f'{counter}/{max_number}. Время {duration:.2f} мин'\n",
    "                save_batch(lines, elastic_endpoint, index_name)\n",
    "                lines.clear()\n",
    "       \n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    finally:\n",
    "        conn.close()    \n",
    "    \n",
    "def create_index(index_name:str, mapping_file:str):\n",
    "    \"\"\" Создает индекс Эластик из маппинга в файле. \n",
    "    - index_name - имя индекса эластик\n",
    "    - mapping_file_name -имя файла с маппингом в формате  JSON.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        f = open(mapping_file, 'r') \n",
    "        json_data = json.load(f)\n",
    "        f.close()\n",
    "        pprint(es.indices.create(index=index_name, body=json_data))\n",
    "    except Exception as ex:\n",
    "        pprint(ex)\n",
    "\n"
   ]
  },
  {
   "source": [
    "## Получить список индексов"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "health status index          uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   articles_tfidf 68CuvhmpQh2jiV32Bp44tw   1   0          0            0       208b           208b\ngreen  open   articles_k20b0 jJI7IwmfSmGGY7EtHdhZZA   1   0          0            0       208b           208b\ngreen  open   articles       gsVg1W2SRZKzmOCxuS2wEw   1   0    1232876            0      9.6gb          9.6gb\n\n"
     ]
    }
   ],
   "source": [
    "print(es.cat.indices('art*', v=True))\n",
    "# articles_indices = [ idx['index'] for idx in es.cat.indices(v=True, format='json') if idx['index'][:3] == 'art' ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создать индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'acknowledged': True, 'index': 'articles_k20b0', 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "create_index(index_name='articles_k20b0', mapping_file='mappings/articles-k20b0-mapping.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'acknowledged': True, 'index': 'articles_tfidf', 'shards_acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "create_index(index_name='articles_tfidf', mapping_file='mappings/articles-tfidf-mapping.json')"
   ]
  },
  {
   "source": [
    "## Удалить индексы\n",
    "<div style=\"font-weight:bold; color:red;\">осторожно</div>\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "es.indices.delete('articles_k20b0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es.indices.delete('articles_tfidf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт таблиц в Эластик \n",
    "\n",
    "Перед тем как сделать импорт создайте индексы с помощью команд приведенных выше. \n",
    "В противном случае будут созданы индексы с маппингом по умолчанию,\n",
    "что не всегда соответствует ожиданиям.\n",
    "\n",
    "Действуйте внимательно, поскольку данные могут быть перезаписаны.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_to_elastic('rubrics', 'id', ELASTIC_ENDPOINT, 'rubrics', max_number=2000 , batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_to_elastic('rubrics_objects', \"kind || '-' || rubric_id || '-' || object_id\", ELASTIC_ENDPOINT, 'rubrics_objects', max_number=3500000 , batch_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_to_elastic('articles', 'obj_id', ELASTIC_ENDPOINT, 'articles', max_number=1250000 , batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_to_elastic('articles', 'obj_id', ELASTIC_ENDPOINT, 'articles_k20b0', max_number=1250000 , batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_table_to_elastic('articles', 'obj_id', ELASTIC_ENDPOINT, 'articles_tfidf', max_number=1250000 , batch_size=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}